{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a188be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "# Tensorflow datasets includes the 'MNIST' dataset, which we need for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a30a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "# tfds.load(name='mnist', as_supervised=True) are actually only necessary arguments\n",
    "# with_info=True returns tuple containing info about version, features and number of samples, which we save into variable\n",
    "# mnist_info\n",
    "# data_dir=\"\" saves MNIST data in a specific disk for future use, first time we run the code it downloads the dataset\n",
    "# afterwards, it is readily available to where we save it.\n",
    "# mnist_dataset variable will now have mnist data training and test values. We don't have validation sample separately\n",
    "# as TF doesn't include it. However this is an opportunity for us to learn how to split training data into validation and training\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', data_dir='C:/', with_info=True, as_supervised=True)\n",
    "\n",
    "# Now we are going to split the mnist_dataset into training and test, mnist dataset already has 'train' and 'test' columns\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# Now, we shall decide how much of the training data we want to be used for validating.Let that number be 10% of all train data.\n",
    "# mnist_info['train'] returns the number of samples in training data and splits it according to our request\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# Now the value that is returned for validation samples, might be in binary, so we need to convert it to integer\n",
    "# For that Purpose we shall use tf.cast(samples, format) takes two arguments: sample we want to 'cast' and data type\n",
    "# Let's transform the data\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# Now let's store num of test samples in a variable\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "# And cast it to integer\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "# Next step is to scale images from 0 to 1, for them to be numerically stable, since rgb colors have numbers assigned from 0 to 255\n",
    "# We will come up with a function that scales them properly. \".\" means that returned value should be float. \n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "# dataset.map(*function*) -takes a function as an input. That function should define the transformation. \n",
    "# It is of utmost importance for function we define for transformation to take image and label as inputs and return img, label.\n",
    "# Below we will be scaling train and validation data. 1 below, we will also scale the test data. \n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "# Another preprocessing step we need to take is to shuffle the data. This is needed because we are going to be batching it later.\n",
    "# We will be choosing BUFFER_SIZE, which indicates how many samples will be shuffled at a time. \n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# We have a function for shuffling our data in tf, so we are just going to use it\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "# We have successfully shuffled our data 10k sample at a time.\n",
    "\n",
    "# Now we will use .take function to extract validation and training samples from shuffled sample\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "# .skip method takes \"everything but\" the sample we specify as an argument\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# We will use 'mini batch method' to split our data into batches\n",
    "BATCH_SIZE = 100\n",
    "# Below operation will add another column to our tensor to indicate how many samples it should take per batch\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "# We won't be batching validation data, because there's no backpropagatain done there, in contrast - \n",
    "# in training data weights and biases will be updated through backpropagataion every 100 samples.(every 1 batch)\n",
    "# We will only overwrite validation data with batch, because our model requires so\n",
    "validation_data = validation_data.batch(num_validation_samples) # single batch. \n",
    "# Same with test_data no batching needed per batch size, we just need to format it for model\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# Our validation data should have same size and object properties as train and test data.\\\n",
    "# making data iterable - iter(), next() loads the inputs and targets\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2c0cea",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2554f5c8",
   "metadata": {},
   "source": [
    "### Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69be516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's time to build our model; We shall start with setting the hyperparameters: width, depth etc.\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 500\n",
    "# We build the model using tf.keras.Sequential([]) | tf.keras.layers.Flatten flattens tensor into vector to use in our model.]\n",
    "# Then we start building as many hidden layers as we want, using \"tf.keras.layers.Dense(layer_size, activation_function)\"\n",
    "# For MNIST, let's use \"relu\" activation as it is advised\n",
    "# We will build 2 hidden layers as advised and then the output layer. In the output layer we will use different activation\n",
    "# function - softmax - which gives outputs probabilities.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), #1st Layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), #2nd Layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # Output Layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc111d",
   "metadata": {},
   "source": [
    "## Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5b3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer, loss) configures our model for training\n",
    "# Best optimizer we got is 'Adaptive moment estimation' aka 'Adam'\n",
    "# This model should solve classification problem, therefore we need to use some kind of crossentropy loss function:\n",
    "# We 've got 3 options: Binary, categorical and sparse_categorical. The last one applies one-hot encoding, which is what we need.\n",
    "# We shall also include 'metrics - accuracy', since we want to calculate how accurate our model is.\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3935abd0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb229e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 7s - loss: 0.2191 - accuracy: 0.9359 - val_loss: 0.1114 - val_accuracy: 0.9658 - 7s/epoch - 13ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 5s - loss: 0.0832 - accuracy: 0.9738 - val_loss: 0.0787 - val_accuracy: 0.9778 - 5s/epoch - 8ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 4s - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0589 - val_accuracy: 0.9832 - 4s/epoch - 7ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 4s - loss: 0.0428 - accuracy: 0.9863 - val_loss: 0.0514 - val_accuracy: 0.9848 - 4s/epoch - 7ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 4s - loss: 0.0320 - accuracy: 0.9894 - val_loss: 0.0541 - val_accuracy: 0.9832 - 4s/epoch - 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b3f283e20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time for the last step - We need to train our model. Let's choose the number of epochs we want our model to learn for.\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Let's now fit everything on our model\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
    "# Let's state what we expect to happen in each epoch before executing:\n",
    "# 1. At the beginning of each epoch, training loss will be set to 0\n",
    "# 2. Algorithm will iterate over a preset number of batches, all from train_data\n",
    "# 3. Weights and Biases will be updated as many times as there are batches.\n",
    "# 4. At the end of each epoch, we will see the training loss to see how our model is doing/learning.\n",
    "# 5. We will also see the training accuracy.\n",
    "# 6. Moreover, at the end of each epoch, the algorithm will forward propagate the validation set to calculate the validation accuracy.\n",
    "# Eventually, when algorithm reaches the total number of epochs, our model will finish training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5d7d0",
   "metadata": {},
   "source": [
    "### We got the validation accuracy of 98.32%, which is solid - there are still ways to improve our model of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ways to improve: Increase batch_size, increase the number of hidden layers, increase the width of a hidden layer dramatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b628f6a",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77169b",
   "metadata": {},
   "source": [
    "### We are now going to add the absolute last step to check our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a22d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 655ms/step - loss: 0.0854 - accuracy: 0.9773\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b2b603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09. Test accuracy: 97.73%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
